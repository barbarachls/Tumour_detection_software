{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuSdRAe0v2f-",
        "outputId": "659cc6e3-6b51-48ec-a35e-865437037e71"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naTCWReRAgEB",
        "outputId": "cab173d7-b99a-46d1-c194-56ff5ebebaa2"
      },
      "outputs": [],
      "source": [
        "!pip install pydicom\n",
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEITQw_eAvBt"
      },
      "outputs": [],
      "source": [
        "# first, load the libraries\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "from copy import deepcopy\n",
        "from torch import nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.io import imread, imsave\n",
        "from skimage.exposure import rescale_intensity\n",
        "from skimage.filters import gaussian\n",
        "from skimage.transform import rotate\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.models import densenet121\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dmP7CkYA4Da",
        "outputId": "7a699981-5c95-4a2f-be32-b9415b166fad"
      },
      "outputs": [],
      "source": [
        "# download the dataset form kaggle ( the data set is pre processed and is private)\n",
        "# my username and APi key is needed\n",
        "\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/barbaracharles/reduced-bc-duke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jonTC7310QRg"
      },
      "outputs": [],
      "source": [
        "class DBCDataset_train(Dataset):\n",
        "    def __init__(self, data_dir, img_size):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # assign labels to data within this Dataset\n",
        "        self.labels = None\n",
        "        self.create_labels()\n",
        "\n",
        "    def create_labels(self):\n",
        "        # create and store a label (positive/1 or negative/0 for each image)\n",
        "        # each label is the tuple: (img filename, label number (0 or 1))\n",
        "        labels = []\n",
        "        print('building DBC dataset labels.')\n",
        "        # iterate over each class\n",
        "        for target, target_label in enumerate(['neg', 'pos']):\n",
        "            case_dir = os.path.join(self.data_dir, target_label)\n",
        "            #counter = 0 # Uncomment to use Dataset E\n",
        "            for fname in os.listdir(case_dir):\n",
        "                if '.png' in fname:\n",
        "                    fpath = os.path.join(case_dir, fname)\n",
        "                    #if counter % 2 == 0: # Uncomment to use Dataset E\n",
        "                    labels.append((fpath, target)) # Add a tab to use Dataset E\n",
        "                    #counter += 1 # Uncomment to use Dataset E\n",
        "\n",
        "        self.labels = labels\n",
        "\n",
        "    def normalize(self, img):\n",
        "        # normalize image pixel values to range [0, 255]\n",
        "        # img expected to be array\n",
        "\n",
        "        # convert uint16 -> float\n",
        "        img = img.astype(np.float32) * 255. / img.max()\n",
        "        # convert float -> unit8\n",
        "        img = img.astype(np.uint8)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # required method for accessing data samples\n",
        "        # returns data with its label\n",
        "        fpath, target = self.labels[idx]\n",
        "\n",
        "        # load img from file (png or jpg)\n",
        "        img_arr = imread(fpath, as_gray=True)\n",
        "\n",
        "        # Apply Gaussian blur randomly to every other image\n",
        "        if idx % 6 == 1:  # Apply Gaussian blur on the second of every 4 images\n",
        "            sigma = random.uniform(0.5, 3.0)  # Randomly select sigma for Gaussian blur\n",
        "            img_arr = gaussian(img_arr, sigma=sigma)\n",
        "            img_arr = (img_arr * 255).astype(np.uint8)\n",
        "\n",
        "        # Increase brightness on the third of every 4 images\n",
        "        elif idx % 6 == 2:\n",
        "            brightness_increase = 0.3\n",
        "            img_arr = rescale_intensity(img_arr, in_range='image', out_range=(0, 1))  # Rescale intensity to range [0, 1]\n",
        "            img_arr += brightness_increase\n",
        "            img_arr = np.clip(img_arr, 0, 1)  # Clip values to ensure they remain within [0, 1] range\n",
        "            img_arr = (img_arr * 255).astype(np.uint8)\n",
        "\n",
        "        # Rotate 25 degrees on the fourth of every 4 images\n",
        "        elif idx % 6 == 3:\n",
        "            img_arr = rotate(img_arr, angle=25)\n",
        "            img_arr = (img_arr * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "        # normalize image\n",
        "        img_arr = img_arr / 255.0\n",
        "\n",
        "        # convert to tensor (PyTorch matrix)\n",
        "        data = torch.from_numpy(img_arr)\n",
        "        data = data.type(torch.FloatTensor)\n",
        "\n",
        "        # add image channel dimension (to work with neural network)\n",
        "        data = torch.unsqueeze(data, 0)\n",
        "\n",
        "        # resize image\n",
        "        data = transforms.Resize((self.img_size, self.img_size))(data)\n",
        "\n",
        "        return data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        # required method for getting size of dataset\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "\n",
        "class DBCDataset(Dataset):\n",
        "    def __init__(self, data_dir, img_size):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # assign labels to data within this Dataset\n",
        "        self.labels = None\n",
        "        self.create_labels()\n",
        "\n",
        "    def create_labels(self):\n",
        "        # create and store a label (positive/1 or negative/0 for each image)\n",
        "        # each label is the tuple: (img filename, label number (0 or 1))\n",
        "        labels = []\n",
        "        print('building DBC dataset labels.')\n",
        "        # iterate over each class\n",
        "        for target, target_label in enumerate(['neg', 'pos']):\n",
        "            case_dir = os.path.join(self.data_dir, target_label)\n",
        "            for fname in os.listdir(case_dir):\n",
        "                if '.png' in fname:\n",
        "                    fpath = os.path.join(case_dir, fname)\n",
        "                    labels.append((fpath, target))\n",
        "\n",
        "        self.labels = labels\n",
        "\n",
        "    def normalize(self, img):\n",
        "        # normalize image pixel values to range [0, 255]\n",
        "        # img expected to be array\n",
        "\n",
        "        # convert uint16 -> float\n",
        "        img = img.astype(np.float32) * 255. / img.max()\n",
        "        # convert float -> unit8\n",
        "        img = img.astype(np.uint8)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # required method for accessing data samples\n",
        "        # returns data with its label\n",
        "        fpath, target = self.labels[idx]\n",
        "\n",
        "        # load img from file (png or jpg)\n",
        "        img_arr = imread(fpath, as_gray=True)\n",
        "\n",
        "        # normalize image\n",
        "        #img_arr = self.normalize(img_arr)\n",
        "\n",
        "        img_arr = img_arr / 255.0\n",
        "\n",
        "        # convert to tensor (PyTorch matrix)\n",
        "        data = torch.from_numpy(img_arr)\n",
        "        data = data.type(torch.FloatTensor)\n",
        "\n",
        "        # add image channel dimension (to work with neural network)\n",
        "        data = torch.unsqueeze(data, 0)\n",
        "\n",
        "        # resize image\n",
        "        data = transforms.Resize((self.img_size, self.img_size))(data)\n",
        "\n",
        "        return data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        # required method for getting size of dataset\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PnR_6aD0VtX",
        "outputId": "3275d3e1-565c-4999-f903-f8840aef0001"
      },
      "outputs": [],
      "source": [
        "# Create the dataset\n",
        "\n",
        "img_size=128\n",
        "\n",
        "train_data_dir='/content/reduced-bc-duke/reduced_dataset/png_out/training'\n",
        "val_data_dir='/content/reduced-bc-duke/reduced_dataset/png_out/validation'\n",
        "test_data_dir='/content/reduced-bc-duke/reduced_dataset/png_out/testing'\n",
        "\n",
        "training_dataset = DBCDataset_train(train_data_dir,img_size)\n",
        "validation_dataset = DBCDataset(val_data_dir,img_size)\n",
        "testing_dataset = DBCDataset(test_data_dir,img_size)\n",
        "\n",
        "print(f\"length of the training dataset: {len(training_dataset)}\")\n",
        "print(f\"length of the validation dataset: {len(validation_dataset)}\")\n",
        "print(f\"length of the testing dataset: {len(testing_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFsUK8p4CSoD",
        "outputId": "d7c15519-5d47-44ce-96ba-0baa5cd26e35"
      },
      "outputs": [],
      "source": [
        "# GPUs # to run only online if a gpu is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('running on {}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC0e7ep0CWrA"
      },
      "outputs": [],
      "source": [
        "# Set the batch sizes and load the dataset\n",
        "\n",
        "train_batchsize = 32\n",
        "eval_batchsize = 8\n",
        "train_loader = DataLoader(training_dataset,\n",
        "                          batch_size=train_batchsize,\n",
        "                          shuffle=True\n",
        "                          # images are loaded in random order\n",
        "                          )\n",
        "\n",
        "validation_loader = DataLoader(validation_dataset,\n",
        "                               batch_size=eval_batchsize,\n",
        "                               shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(testing_dataset,\n",
        "                         batch_size=eval_batchsize)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16lVJhvACZ2r"
      },
      "outputs": [],
      "source": [
        "# set random seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96M4EjGiC7Ew"
      },
      "outputs": [],
      "source": [
        "#Load the model and set the hyperparameters\n",
        "\n",
        "model = densenet121(pretrained=False)\n",
        "\n",
        "# Change the input size of the architecture\n",
        "model.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "weight_decay=0.0001\n",
        "#error_minimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=weight_decay)\n",
        "error_minimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "# Set up the learning rate scheduler\n",
        "lr_scheduler = ReduceLROnPlateau(error_minimizer, mode='min', patience=1, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POPObrg9PAGu"
      },
      "outputs": [],
      "source": [
        "# Create log files to be able to rereive trainig information later\n",
        "\n",
        "!mkdir -p \"/content/drive/MyDrive/Densenet\"\n",
        "model_name = f\"Densenet_{train_batchsize}_{eval_batchsize}_{epochs}_{learning_rate}_{weight_decay}_{img_size}_Adam\"\n",
        "\n",
        "filename = f'{model_name}.txt'\n",
        "log_epoch = f'{model_name}_epoch.txt'\n",
        "dir_path = f'/content/drive/MyDrive/Densenet/{model_name}'\n",
        "!mkdir -p $dir_path\n",
        "log_file_path = os.path.join(dir_path, filename)\n",
        "epoch_log_file_path = os.path.join(dir_path, log_epoch)\n",
        "\n",
        "with open(log_file_path, \"w\") as f:\n",
        "  f.write(f'Model : Densenet, number of epochs: {epochs}, training batch size: {train_batchsize}, eval batch size: {eval_batchsize}, error minimiser learning rate: {learning_rate}\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4qFbNhuDJSi",
        "outputId": "0d7a31cf-848a-4584-8e95-765986325775"
      },
      "outputs": [],
      "source": [
        "# Training and validation of the model\n",
        "\n",
        "model_final = deepcopy(model)\n",
        "\n",
        "best_val_accuracy = 0.0\n",
        "best_val_loss = float('inf')  # Initialize with infinity\n",
        "epochs_without_improvement = 0\n",
        "max_epochs_without_improvement = 2\n",
        "# used to pick the best-performing model on the validation set\n",
        "\n",
        "# for training visualization later\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "\n",
        "# for storing loss function values\n",
        "train_loss = []  # list to store training losses\n",
        "val_loss = []    # list to store validation losses\n",
        "\n",
        "# for storing specificity and sensitivity\n",
        "train_specificity = []\n",
        "train_sensitivity = []\n",
        "val_specificity = []\n",
        "val_sensitivity = []\n",
        "\n",
        "# training loop\n",
        "for epoch in range(epochs):\n",
        "    # set network to training mode, so that its parameters can be changed\n",
        "    model.train()\n",
        "\n",
        "    # print training info\n",
        "    print(f\"### Epoch {epoch}:\")\n",
        "    with open(log_file_path, \"a\") as f:\n",
        "      f.write(f\"### Epoch {epoch}:\\n\")\n",
        "    with open(epoch_log_file_path, \"a\") as f:\n",
        "      f.write(f\"### Epoch {epoch}:\\n\")\n",
        "\n",
        "    # statistics needed to compute classification accuracy:\n",
        "    # the total number of image examples trained on\n",
        "    total_train_examples = 0\n",
        "\n",
        "    # the number of examples classified correctly\n",
        "    num_correct_train = 0\n",
        "\n",
        "    # variable to store training loss in this epoch\n",
        "    epoch_train_loss = 0.0\n",
        "\n",
        "    # variables for specificity and sensitivity\n",
        "    true_positives_train = 0\n",
        "    true_negatives_train = 0\n",
        "    false_positives_train = 0\n",
        "    false_negatives_train = 0\n",
        "\n",
        "    # iterate over the training set once\n",
        "    for batch_index, (inputs, targets) in tqdm(enumerate(train_loader),\n",
        "                                               total=len(training_dataset) // train_batchsize):\n",
        "        # load the data onto the computation device.\n",
        "        # inputs are a tensor of shape:\n",
        "        #   (batch size, number of channels, image height, image width).\n",
        "        # targets are a tensor of one-hot-encoded class labels for the inputs,\n",
        "        #   of shape (batch size, number of classes)\n",
        "        # in other words,\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # reset changes (gradients) to parameters\n",
        "        error_minimizer.zero_grad()\n",
        "\n",
        "        # get the network's predictions on the training set batch\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        # evaluate the error, and estimate\n",
        "        #   how much to change the network parameters\n",
        "        loss = criterion(predictions, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Uncomment to use gradient clipping and adjust the norm is needed\n",
        "\n",
        "\n",
        "        # change parameters\n",
        "        error_minimizer.step()\n",
        "\n",
        "        # accumulate loss for this epoch\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "        # calculate predicted class label\n",
        "        # the .max() method returns the maximum entries, and their indices;\n",
        "        # we just need the index with the highest probability,\n",
        "        #   not the probability itself.\n",
        "        _, predicted_class = predictions.max(1)\n",
        "        total_train_examples += predicted_class.size(0)\n",
        "\n",
        "        # calculate true positives, true negatives, false positives, and false negatives\n",
        "        true_positives_train += ((predicted_class == 1) & (targets == 1)).sum().item()\n",
        "        true_negatives_train += ((predicted_class == 0) & (targets == 0)).sum().item()\n",
        "        false_positives_train += ((predicted_class == 1) & (targets == 0)).sum().item()\n",
        "        false_negatives_train += ((predicted_class == 0) & (targets == 1)).sum().item()\n",
        "\n",
        "        with open(epoch_log_file_path, \"a\") as f:\n",
        "          f.write(f\"Batch {batch_index + 1}/{len(train_loader)} - Loss: {loss.item()}\\n\")\n",
        "\n",
        "    # calculate specificity and sensitivity\n",
        "    epoch_train_specificity = true_negatives_train / (true_negatives_train + false_positives_train)\n",
        "    epoch_train_sensitivity = true_positives_train / (true_positives_train + false_negatives_train)\n",
        "    # store specificity and sensitivity\n",
        "    train_specificity.append(epoch_train_specificity)\n",
        "    train_sensitivity.append(epoch_train_sensitivity)\n",
        "\n",
        "\n",
        "    # get results\n",
        "    # total prediction accuracy of network on training set\n",
        "    epoch_train_accuracy = (true_positives_train+true_negatives_train)/total_train_examples\n",
        "    print(f\"Training accuracy: {epoch_train_accuracy}\")\n",
        "    with open(log_file_path, \"a\") as f:\n",
        "      f.write(f\"Training accuracy: {epoch_train_accuracy}\\n\")\n",
        "    train_accuracy.append(epoch_train_accuracy)\n",
        "\n",
        "    # average training loss for this epoch\n",
        "    epoch_train_loss /= len(train_loader)\n",
        "    train_loss.append(epoch_train_loss)\n",
        "\n",
        "    # predict on validation set (similar to training set):\n",
        "    total_val_examples = 0\n",
        "    num_correct_val = 0\n",
        "\n",
        "    # variable to store validation loss in this epoch\n",
        "    epoch_val_loss = 0.0\n",
        "\n",
        "    # variables for specificity and sensitivity\n",
        "    true_positives_val = 0\n",
        "    true_negatives_val = 0\n",
        "    false_positives_val = 0\n",
        "    false_negatives_val = 0\n",
        "\n",
        "    # switch network from training mode (parameters can be trained)\n",
        "    #   to evaluation mode (parameters can't be trained)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():  # don't save parameter changes\n",
        "        #                      since this is not for training\n",
        "        for batch_index, (inputs, targets) in tqdm(enumerate(validation_loader),\n",
        "                                                   total=len(validation_dataset) // eval_batchsize):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            predictions = model(inputs)\n",
        "\n",
        "            _, predicted_class = predictions.max(1)\n",
        "            total_val_examples += predicted_class.size(0)\n",
        "            num_correct_val += predicted_class.eq(targets).sum().item()\n",
        "\n",
        "            # calculate validation loss\n",
        "            loss = criterion(predictions, targets)\n",
        "            epoch_val_loss += loss.item()\n",
        "\n",
        "            # calculate true positives, true negatives, false positives, and false negatives\n",
        "            true_positives_val += ((predicted_class == 1) & (targets == 1)).sum().item()\n",
        "            true_negatives_val += ((predicted_class == 0) & (targets == 0)).sum().item()\n",
        "            false_positives_val += ((predicted_class == 1) & (targets == 0)).sum().item()\n",
        "            false_negatives_val += ((predicted_class == 0) & (targets == 1)).sum().item()\n",
        "\n",
        "    # calculate specificity and sensitivity\n",
        "    epoch_val_specificity = true_negatives_val / (true_negatives_val + false_positives_val)\n",
        "    epoch_val_sensitivity = true_positives_val / (true_positives_val + false_negatives_val)\n",
        "    # store specificity and sensitivity\n",
        "    val_specificity.append(epoch_val_specificity)\n",
        "    val_sensitivity.append(epoch_val_sensitivity)\n",
        "\n",
        "    # get results\n",
        "    # total prediction accuracy of network on validation set\n",
        "    epoch_val_accuracy = (true_positives_val + true_negatives_val) / total_val_examples\n",
        "    print(f\"Validation accuracy: {epoch_val_accuracy}\")\n",
        "    val_accuracy.append(epoch_val_accuracy)\n",
        "    with open(log_file_path, \"a\") as f:\n",
        "      f.write(f\"Validation accuracy: {epoch_val_accuracy}\\n\")\n",
        "\n",
        "    # average validation loss for this epoch\n",
        "    epoch_val_loss /= len(validation_loader)\n",
        "    val_loss.append(epoch_val_loss)\n",
        "\n",
        "    # Finally, save model if the validation accuracy is the best so far\n",
        "    if epoch_val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = epoch_val_accuracy\n",
        "        best_train_accuracy = epoch_train_accuracy\n",
        "        best_train_specificity = epoch_train_specificity\n",
        "        best_train_sensitivity = epoch_train_sensitivity\n",
        "        best_val_specificity = epoch_val_specificity\n",
        "        best_val_sensitivity = epoch_val_sensitivity\n",
        "        print(\"Validation accuracy improved; saving model.\")\n",
        "        with open(log_file_path, \"a\") as f:\n",
        "          f.write(\"Validation accuracy improved; saving model.\\n\")\n",
        "        model_final = deepcopy(model)\n",
        "\n",
        "    # Get the current learning rate\n",
        "    with open(log_file_path, \"a\") as f:\n",
        "        f.write(f\"Current learning rate: {error_minimizer.param_groups[0]['lr']}\\n\")\n",
        "    # Reduce the learnign rate\n",
        "    val_loss_avg = epoch_val_loss / len(validation_loader)\n",
        "    lr_scheduler.step(val_loss_avg)\n",
        "\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "    # If validation loss hasn't improved for 'max_epochs_without_improvement' epochs, stop training\n",
        "    if epochs_without_improvement >= max_epochs_without_improvement:\n",
        "        print(f\"No improvement in validation loss for {max_epochs_without_improvement} epochs. Stopping early.\")\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(log_file_path, \"a\") as f:\n",
        "  f.write(f\"Training Accuracies: {train_accuracy}\\n\")\n",
        "  f.write(f\"Training Specificities: {train_specificity}\\n\")\n",
        "  f.write(f\"Training Sensitivities: {train_sensitivity}\\n\")\n",
        "  f.write(f\"Traingin Loss: {train_loss}\\n\")\n",
        "  f.write(f\"Validation Accuracies: {val_accuracy}\\n\")\n",
        "  f.write(f\"Validation Specificities: {val_specificity}\\n\")\n",
        "  f.write(f\"Validation Sensitivities: {val_sensitivity}\\n\")\n",
        "  f.write(f\"Validation Loss: {val_loss}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMSHKwbDHis6"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model_save_name = f'{model_name}.pt'\n",
        "path = F\"/content/drive/MyDrive/{model_save_name}\"\n",
        "torch.save(model_final.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JxdNIuuMDM4O",
        "outputId": "ad33bff1-8f0e-4a8a-cc6d-5fee01234d2c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracy, label='Training Accuracy', color='blue')\n",
        "plt.plot(val_accuracy, label='Validation Accuracy', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "filename = f'Acc_{model_name}.png'\n",
        "graph_file_path = os.path.join(dir_path, filename)\n",
        "plt.savefig(graph_file_path)\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss, label='Training Loss', color='blue')\n",
        "plt.plot(val_loss, label='Validation Loss', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "filename = f'Loss_{model_name}.png'\n",
        "graph_file_path = os.path.join(dir_path, filename)\n",
        "plt.savefig(graph_file_path)\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation specificity\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_specificity, label='Training Specificity', color='blue')\n",
        "plt.plot(val_specificity, label='Validation Specificity', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Specificity')\n",
        "plt.title('Training and Validation Specificity')\n",
        "plt.legend()\n",
        "\n",
        "filename = f'Spec_{model_name}.png'\n",
        "graph_file_path = os.path.join(dir_path, filename)\n",
        "plt.savefig(graph_file_path)\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation sensitivity\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_sensitivity, label='Training Sensitivity', color='blue')\n",
        "plt.plot(val_sensitivity, label='Validation Sensitivity', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Sensitivity')\n",
        "plt.title('Training and Validation Sensitivity')\n",
        "plt.legend()\n",
        "\n",
        "filename = f'Sens_{model_name}.png'\n",
        "graph_file_path = os.path.join(dir_path, filename)\n",
        "plt.savefig(graph_file_path)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "P6r1h4xPE3Xt",
        "outputId": "5c19c142-f466-4709-8070-522f40e6de59"
      },
      "outputs": [],
      "source": [
        "# for data visualization\n",
        "total_test_examples = 0\n",
        "num_correct_test = 0\n",
        "\n",
        "true_positives_test = 0\n",
        "false_positives_test = 0\n",
        "true_negatives_test = 0\n",
        "false_negatives_test = 0\n",
        "\n",
        "# see how well the final trained model does on the test set\n",
        "with torch.no_grad():  # don't save parameter gradients/changes since this is not for model training\n",
        "    for batch_index, (inputs, targets) in enumerate(test_loader):\n",
        "        # make predictions\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        predictions = model_final(inputs)\n",
        "\n",
        "        # compute prediction statistics\n",
        "        _, predicted_class = predictions.max(1)\n",
        "        total_test_examples += predicted_class.size(0)\n",
        "        num_correct_test += predicted_class.eq(targets).sum().item()\n",
        "\n",
        "        # thanks to\n",
        "        #   https://gist.github.com/the-bass/cae9f3976866776dea17a5049013258d\n",
        "        confusion_vector = predicted_class / targets\n",
        "        # num_true_pos = torch.sum(confusion_vector == 1).item()\n",
        "        # num_false_pos = torch.sum(confusion_vector == float('inf')).item()\n",
        "\n",
        "        true_positive = torch.sum(confusion_vector == 1).item()\n",
        "        false_positive = torch.sum(confusion_vector == float('inf')).item()\n",
        "        true_negative = torch.sum(torch.isnan(confusion_vector)).item()\n",
        "        false_negative = torch.sum(confusion_vector == 0).item()\n",
        "\n",
        "        true_positives_test += true_positive\n",
        "        false_positives_test += false_positive\n",
        "        true_negatives_test += true_negative\n",
        "        false_negatives_test += false_negative\n",
        "\n",
        "\n",
        "# get total results\n",
        "# total prediction accuracy of network on test set\n",
        "test_accuracy = (true_positives_test + true_negatives_test) / total_test_examples\n",
        "test_specificity = true_negatives_test / (true_negatives_test + false_positives_test + 1e-8)\n",
        "test_sensitivity = true_positives_test / (true_positives_test + false_negatives_test + 1e-8)\n",
        "print(f\"Test set accuracy: {test_accuracy}\\n\")\n",
        "print(f\"Test set specificity: {test_specificity}\\n\")\n",
        "print(f\"Test set sensitivity: {test_sensitivity}\\n\")\n",
        "print(f\"{true_positives_test} true positive classifications,\\n {false_positives_test} false positive classifications\\n\")\n",
        "print(f\"{true_negatives_test} true negative classification,\\n {false_negatives_test}false negative classification\\n\")\n",
        "\n",
        "with open(log_file_path, \"a\") as f:\n",
        "    f.write(f\"Test set accuracy: {test_accuracy}\\n\")\n",
        "    f.write(f\"Test set specificity: {test_specificity}\\n\")\n",
        "    f.write(f\"Test set sensitivity: {test_sensitivity}\\n\")\n",
        "    f.write(f\"{true_positives_test} true positive classifications,\\n {false_positives_test} false positive classifications\\n\")\n",
        "    f.write(f\"{true_negatives_test} true negative classification,\\n {false_negatives_test}false negative classification\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjjxnbaJlpaG"
      },
      "outputs": [],
      "source": [
        "#Model\tTraining batch\tEval batch\tepochs Learning rate\ttest acc\ttest spec\ttest sens\ttrain acc\ttrain spec\ttrain sens\tval acc\tval spec\tval sens\n",
        "data = ['Densenet', train_batchsize, eval_batchsize, epochs, learning_rate, test_accuracy, test_specificity, test_sensitivity, best_train_accuracy, best_train_specificity, best_train_sensitivity,  best_val_accuracy, best_val_specificity, best_val_sensitivity ]\n",
        "\n",
        "\n",
        "results_file_path = \"/content/drive/MyDrive/ResultsClass.csv\"\n",
        "\n",
        "with open(results_file_path, mode='a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Write each row of new data to the CSV file\n",
        "    writer.writerow(data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
